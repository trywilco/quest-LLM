{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d71479",
   "metadata": {},
   "source": [
    "# Word Prediction with GPT-2\n",
    "\n",
    "Welcome to your word prediction adventure! In this notebook, we'll explore how Large Language Models can predict and complete text sequences using GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1fde3",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's install and import the necessary libraries. We'll use the Hugging Face Transformers library for easy access to GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4683bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31abe8",
   "metadata": {},
   "source": [
    "## Load the GPT-2 Model\n",
    "\n",
    "Let's load a pre-trained GPT-2 model. We'll use the small version for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also try \"gpt2-medium\" for better quality\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(f\"GPT-2 model '{model_name}' loaded successfully!\")\n",
    "print(f\"Model has {model.config.n_embd} dimensions and {model.config.n_layer} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a8f6d",
   "metadata": {},
   "source": [
    "## Basic Word Prediction Function\n",
    "\n",
    "Let's create a simple function to generate text completions with customizable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(prompt, max_new_tokens=10, temperature=1.0, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text completion using GPT-2\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input text to complete\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate (excluding prompt)\n",
    "        temperature (float): Controls randomness (0.1 = conservative, 2.0 = creative)\n",
    "        num_return_sequences (int): Number of different completions to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of generated text completions\n",
    "    \"\"\"\n",
    "    # Generate text\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Extract generated text\n",
    "    completions = []\n",
    "    for output in outputs:\n",
    "        generated_text = output['generated_text']\n",
    "        # Remove the original prompt to show only the completion\n",
    "        completion = generated_text[len(prompt):].strip()\n",
    "        completions.append(completion)\n",
    "    \n",
    "    return completions\n",
    "\n",
    "print(\"Text prediction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d822c25",
   "metadata": {},
   "source": [
    "## Explore Different Parameters\n",
    "\n",
    "Try experimenting with different settings to see how they affect the predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa338d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different temperatures\n",
    "test_prompt = \"In the year 2030, technology will\"\n",
    "\n",
    "temperatures = [0.2, 0.8, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    completion = predict_text(test_prompt, max_new_tokens=10, temperature=temp)[0]\n",
    "    print(f\"Temperature {temp}: '{test_prompt} {completion}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebbf6a",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Your Mission: Experiment with Word Prediction\n",
    "\n",
    "Now it's time to experiment! Try the prompt \"The future of artificial intelligence is\" with temperature=0.8 and see what creative completion the model generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "temperature = 0\n",
    "\n",
    "# Generate completion\n",
    "completions = predict_text(prompt, max_new_tokens=60, temperature=temperature)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(\"\\nGenerated completion:\")\n",
    "print(f\"'{completions[0]}'\")\n",
    "\n",
    "# Show the full generated text\n",
    "print(\"\\nFull text:\")\n",
    "print(f\"'{prompt} {completions[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a3b076",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully explored word prediction with GPT-2! You've learned how:\n",
    "- Language models can complete text based on context\n",
    "- Temperature affects creativity vs. consistency\n",
    "- The same prompt can generate different completions\n",
    "- Context and prompt engineering influence the quality of predictions\n",
    "\n",
    "Feel free to continue experimenting with different prompts and parameters to see what interesting completions you can generate!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
