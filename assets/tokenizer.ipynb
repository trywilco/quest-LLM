{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8fb62c",
   "metadata": {},
   "source": [
    "# Tokenization Exploration\n",
    "\n",
    "Welcome to the tokenization exploration notebook! This notebook will help you understand how different tokenizers work and how they convert text into tokens that language models can process.\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, subwords, or characters, depending on the tokenizer used. Different tokenizers have different strategies for splitting text, which can significantly impact how language models interpret and process the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ca6d8",
   "metadata": {},
   "source": [
    "## Installing Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries for tokenization exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (it might take a few minutes)\n",
    "%pip install transformers torch ipywidgets pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4f066",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Let's import the libraries we'll need for tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9520fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, BertTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a30367",
   "metadata": {},
   "source": [
    "## GPT-2 Tokenizer\n",
    "\n",
    "Let's start with the GPT-2 tokenizer, which uses Byte Pair Encoding (BPE). This is one of the most commonly used tokenizers in modern language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2927fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Test sentence from the quest\n",
    "test_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "print(f\"Original sentence: {test_sentence}\")\n",
    "print(f\"Length in characters: {len(test_sentence)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c2fcc",
   "metadata": {},
   "source": [
    "### Tokenizing with GPT-2\n",
    "\n",
    "Now let's see how GPT-2 tokenizes our test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "tokens = gpt2_tokenizer.encode(test_sentence)\n",
    "token_strings = gpt2_tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "print(f\"GPT-2 Tokenization Results:\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Token strings: {token_strings}\")\n",
    "print()\n",
    "\n",
    "# Show each token with its ID\n",
    "print(\"Token breakdown:\")\n",
    "for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
    "    print(f\"  {i+1:2d}. ID: {token_id:5d} | Token: '{token_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f42e45",
   "metadata": {},
   "source": [
    "### Understanding GPT-2 Tokenization\n",
    "\n",
    "Notice how GPT-2 handles different parts of the sentence. The tokenizer uses BPE, which means it can split words into subword units. Let's explore this further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d39940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze each word individually\n",
    "words = test_sentence.split()\n",
    "\n",
    "print(\"Word-by-word tokenization analysis:\")\n",
    "total_tokens = 0\n",
    "\n",
    "for word in words:\n",
    "    word_tokens = gpt2_tokenizer.encode(word)\n",
    "    word_token_strings = gpt2_tokenizer.convert_ids_to_tokens(word_tokens)\n",
    "    total_tokens += len(word_tokens)\n",
    "    \n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    print(f\"  Tokens: {len(word_tokens)}\")\n",
    "    print(f\"  Token strings: {word_token_strings}\")\n",
    "\n",
    "print(f\"\\nTotal tokens when processed word-by-word: {total_tokens}\")\n",
    "print(f\"Tokens when processed as full sentence: {len(tokens)}\")\n",
    "print(f\"Difference: {total_tokens - len(tokens)} (context matters!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08115a",
   "metadata": {},
   "source": [
    "## Comparing Different Tokenizers\n",
    "\n",
    "Let's compare how different tokenizers handle the same sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2604225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different tokenizers\n",
    "tokenizers = {\n",
    "    'GPT-2': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "    'BERT': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "    'RoBERTa': AutoTokenizer.from_pretrained('roberta-base')\n",
    "}\n",
    "\n",
    "# Compare tokenization results\n",
    "comparison_results = []\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokens = tokenizer.encode(test_sentence)\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Tokenizer': name,\n",
    "        'Token Count': len(tokens),\n",
    "        'Tokens': token_strings\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name} Tokenizer:\")\n",
    "    print(f\"  Number of tokens: {len(tokens)}\")\n",
    "    print(f\"  Tokens: {token_strings}\")\n",
    "\n",
    "# Create a comparison dataframe\n",
    "df_comparison = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(df_comparison[['Tokenizer', 'Token Count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5fe8f3",
   "metadata": {},
   "source": [
    "## Visualizing Token Counts\n",
    "\n",
    "Let's create a visual comparison of token counts across different tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot comparing token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df_comparison['Tokenizer'], df_comparison['Token Count'])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, df_comparison['Token Count']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             str(count), ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('Token Count Comparison Across Different Tokenizers\\n\"The quick brown fox jumps over the lazy dog.\"', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Tokenizer', fontsize=12)\n",
    "plt.ylabel('Number of Tokens', fontsize=12)\n",
    "plt.ylim(0, max(df_comparison['Token Count']) + 2)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370655ab",
   "metadata": {},
   "source": [
    "## Interactive Tokenization\n",
    "\n",
    "Now you can experiment with your own sentences! Try different inputs and see how the tokenizers handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa95256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenization(sentence, tokenizer_name='GPT-2'):\n",
    "    \"\"\"\n",
    "    Analyze tokenization for a given sentence\n",
    "    \"\"\"\n",
    "    tokenizer = tokenizers[tokenizer_name]\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    \n",
    "    print(f\"\\n{tokenizer_name} Tokenization Analysis:\")\n",
    "    print(f\"Original sentence: '{sentence}'\")\n",
    "    print(f\"Character length: {len(sentence)}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print(f\"Tokens: {token_strings}\")\n",
    "    print(\"\\nToken breakdown:\")\n",
    "    for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
    "        print(f\"  {i+1:2d}. '{token_str}'\")\n",
    "    \n",
    "    return len(tokens)\n",
    "\n",
    "# Try some examples\n",
    "test_sentences = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Tokenization is fundamental to natural language processing.\",\n",
    "    \"ðŸ¤– AI models use tokenization to understand text! ðŸš€\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    analyze_tokenization(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0636f",
   "metadata": {},
   "source": [
    "## Your Turn: Complete the Quest!\n",
    "\n",
    "Now it's time to answer the quest question. Run the cell below to get the exact token count for the GPT-2 tokenizer on our test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_sentence = \"\"\n",
    "gpt2_tokens = gpt2_tokenizer.encode(quest_sentence)\n",
    "token_count = len(gpt2_tokens)\n",
    "\n",
    "print(f\"ðŸŽ¯ QUEST ANSWER:\")\n",
    "print(f\"Sentence: '{quest_sentence}'\")\n",
    "print(f\"GPT-2 Token Count: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab407ae",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Different tokenizers produce different results** - The same sentence can have different token counts depending on the tokenizer used.\n",
    "\n",
    "2. **Context matters** - Tokenizing a full sentence might produce different results than tokenizing words individually.\n",
    "\n",
    "3. **BPE is efficient** - Byte Pair Encoding (used by GPT-2) creates a good balance between vocabulary size and representation efficiency.\n",
    "\n",
    "4. **Special characters matter** - Punctuation, spaces, and special characters are handled differently by different tokenizers.\n",
    "\n",
    "5. **Tokenization affects model performance** - The choice of tokenizer can significantly impact how well a language model performs on different tasks.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand tokenization, you're ready to explore how these tokens are processed by language models. The tokenization step is crucial because it determines how the model \"sees\" and interprets text input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
