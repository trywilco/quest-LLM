{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056f5de6",
   "metadata": {},
   "source": [
    "# Question Answering with Wikipedia Pages\n",
    "\n",
    "Welcome to your question-answering adventure! In this notebook, we'll build a system that can fetch entire Wikipedia pages and answer questions about them using pre-trained language models.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to fetch and process Wikipedia pages\n",
    "- How to use pre-trained BERT models for question answering on long documents\n",
    "- How to build a practical Q&A system for real-world content\n",
    "- How to handle large text documents that exceed model context limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c6af0",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's install and import the necessary libraries. We'll use Wikipedia API to fetch pages and Transformers for Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install transformers torch wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import wikipedia\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186675b",
   "metadata": {},
   "source": [
    "## Load the Question Answering Model\n",
    "\n",
    "Let's load a pre-trained model optimized for question answering. We'll use DistilBERT for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f69adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\",\n",
    "    tokenizer=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "\n",
    "print(\"Question-answering model loaded successfully!\")\n",
    "print(\"Model: DistilBERT fine-tuned on SQuAD dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ee46b",
   "metadata": {},
   "source": [
    "## Wikipedia Q&A System\n",
    "\n",
    "Now let's build the core system that can fetch Wikipedia pages and answer questions about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(context, question):\n",
    "    \"\"\"\n",
    "    Answer a question based on the given context.\n",
    "    \n",
    "    Args:\n",
    "        context (str): The text passage containing the information\n",
    "        question (str): The question to answer\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains the answer, confidence score, and position\n",
    "    \"\"\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result\n",
    "\n",
    "def fetch_wikipedia_page(title):\n",
    "    \"\"\"Fetch Wikipedia page content by title.\"\"\"\n",
    "    try:\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        page = wikipedia.page(title)\n",
    "        return page.content\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def split_text_into_chunks(text, max_length=400):\n",
    "    \"\"\"Split text into chunks that fit within model limits.\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_length * 4:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def answer_question_from_wikipedia(page_title, question):\n",
    "    \"\"\"Answer a question using content from a Wikipedia page.\"\"\"\n",
    "    # Fetch the page\n",
    "    content = fetch_wikipedia_page(page_title)\n",
    "    if not content:\n",
    "        return {\"error\": \"Could not fetch Wikipedia page\"}\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = split_text_into_chunks(content)\n",
    "    \n",
    "    # Find the best answer across all chunks\n",
    "    best_answer = None\n",
    "    best_score = 0.0  # Explicitly initialize as float\n",
    "    best_chunk_idx = -1\n",
    "    best_result = None\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            result = qa_pipeline(question=question, context=chunk)\n",
    "            # Extract score and ensure it's a proper float\n",
    "            score = float(result['score'])\n",
    "            \n",
    "            # Validate score is in expected range [0, 1]\n",
    "            if score < 0 or score > 1:\n",
    "                print(f\"WARNING: Score {score} is outside [0,1] range in chunk {i}\")\n",
    "                continue\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_answer = result['answer']\n",
    "                best_chunk_idx = i\n",
    "                best_result = result\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Final validation of best score\n",
    "    if best_score > 1.0:\n",
    "        print(f\"ERROR: Final best_score {best_score} is > 1.0! This shouldn't happen.\")\n",
    "        best_score = min(best_score, 1.0)  # Clamp to 1.0 as safety measure\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': best_answer,\n",
    "        'confidence': best_score,\n",
    "        'chunk_index': best_chunk_idx,\n",
    "        'total_chunks': len(chunks)\n",
    "    }\n",
    "\n",
    "print(\"Q&A system ready!\")\n",
    "print(\"You can now use:\")\n",
    "print(\"- answer_question(context, question) for basic Q&A\")\n",
    "print(\"- answer_question_from_wikipedia(page_title, question) for Wikipedia Q&A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378ddb7",
   "metadata": {},
   "source": [
    "## Test the Q&A System\n",
    "\n",
    "Let's test the basic Q&A function with different types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic Q&A with a simple example\n",
    "test_context = \"\"\"\n",
    "The Transformer architecture was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. \n",
    "It revolutionized natural language processing by using self-attention mechanisms instead of recurrent neural networks. \n",
    "The model consists of an encoder and decoder, each made up of multiple layers with multi-head attention and feed-forward networks.\n",
    "\"\"\"\n",
    "\n",
    "test_question = \"When was the Transformer architecture introduced?\"\n",
    "\n",
    "print(\"=== BASIC Q&A TEST ===\")\n",
    "result = answer_question(test_context, test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['score']:.4f}\")\n",
    "print(f\"Position: {result['start']}-{result['end']}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of questions on a more complex context\n",
    "llm_context = \"\"\"\n",
    "Large Language Models (LLMs) are AI systems trained on vast amounts of text data to understand and generate human-like text. \n",
    "Popular examples include GPT-3, GPT-4, BERT, and T5. These models use transformer architectures and are pre-trained on \n",
    "diverse internet text before being fine-tuned for specific tasks. LLMs can perform various tasks including text generation, \n",
    "question answering, summarization, and translation. They typically have billions of parameters and require significant \n",
    "computational resources for training and inference. The training process involves predicting the next word in a sequence, \n",
    "which helps the model learn language patterns, grammar, and world knowledge.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What are some examples of Large Language Models?\",\n",
    "    \"What architecture do LLMs use?\",\n",
    "    \"How many parameters do LLMs typically have?\",\n",
    "    \"What tasks can LLMs perform?\",\n",
    "    \"What is the training process for LLMs?\"\n",
    "]\n",
    "\n",
    "print(\"=== TESTING DIFFERENT QUESTION TYPES ===\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    result = answer_question(llm_context, question)\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.4f}\")\n",
    "    print(f\"Position: {result['start']}-{result['end']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6c539",
   "metadata": {},
   "source": [
    "## Interactive Q&A Session\n",
    "\n",
    "Now let's create an interactive session where you can ask questions about any text passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_qa():\n",
    "    \"\"\"\n",
    "    Interactive question-answering session.\n",
    "    \"\"\"\n",
    "    print(\"Interactive Q&A Session\")\n",
    "    print(\"=\" * 30)\n",
    "    print(\"Enter your context (text passage) and questions.\")\n",
    "    print(\"Type 'quit' to exit, 'new context' to change the passage.\")\n",
    "    \n",
    "    # Default context about AI and Machine Learning\n",
    "    context = \"\"\"\n",
    "    Artificial Intelligence (AI) is a branch of computer science that aims to create machines capable of intelligent behavior. \n",
    "    Machine Learning (ML) is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed. \n",
    "    Deep Learning is a subset of ML that uses neural networks with multiple layers to model and understand complex patterns in data. \n",
    "    Natural Language Processing (NLP) is a field of AI that focuses on the interaction between computers and human language, \n",
    "    enabling machines to understand, interpret, and generate human language.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nCurrent context: {context[:100]}...\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        elif question.lower() == 'new context':\n",
    "            context = input(\"Enter new context: \")\n",
    "            print(f\"Context updated: {context[:100]}...\")\n",
    "            continue\n",
    "        \n",
    "        if question.strip():\n",
    "            try:\n",
    "                result = answer_question(context, question)\n",
    "                print(f\"\\nQuestion: {question}\")\n",
    "                print(f\"Answer: {result['answer']}\")\n",
    "                print(f\"Confidence: {result['score']:.4f}\")\n",
    "                print(f\"Position: {result['start']}-{result['end']}\")\n",
    "                print(\"-\" * 50)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"Please try a different question.\")\n",
    "\n",
    "# Uncomment the line below to start interactive session\n",
    "# interactive_qa()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c9202",
   "metadata": {},
   "source": [
    "## Your Task: Test with Space Race\n",
    "\n",
    "**Your mission**: Use the system to answer any question using the Space Race Wikipedia page.\n",
    "\n",
    "Run the code below and report what answer you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb88fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your test - answer the specific question about the Space Race\n",
    "your_question = \"\"\n",
    "result = answer_question_from_wikipedia(\"Race to space\", your_question)\n",
    "\n",
    "print(\"YOUR RESULT:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "print(f\"Found in chunk {result['chunk_index']} of {result['total_chunks']}\")\n",
    "print(\"\\nThis is the answer you should report back!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ba46b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully built a question-answering system using pre-trained language models. \n",
    "\n",
    "### Key Takeaways:\n",
    "- **Context Understanding**: LLMs can understand context and extract precise information\n",
    "- **Model Comparison**: Different models have different strengths and trade-offs\n",
    "- **Real-world Applications**: Q&A systems have many practical uses\n",
    "- **Experimentation**: Testing with different contexts and questions reveals model capabilities\n",
    "\n",
    "You now have the foundation to build more sophisticated question-answering systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
